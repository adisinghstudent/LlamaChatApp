# Local LLM friend.

This project is a locally hosted AI chatbot application using Llama 3.1 through Ollamma, React Native (Expo), Node.js. The chatbot can be finetuned to your liking and you are free to pick your llm.

## Table of Contents

- [Prerequisites](#prerequisites)
- [Installation](#installation)
- [Running the Application](#running-the-application)
- [Project Structure](#project-structure)
- [Usage](#usage)
- [Contributing](#contributing)
- [License](#license)

## Prerequisites

Before you begin, ensure you have the following installed on your machine:

- Node.js (>=14.x)
- npm (>=6.x)
- Expo CLI (>=4.x)
- Ollama

## Installation

1. **Clone the repository:**

   ```sh
   git clone https://github.com/adisinghstudent/LlamaChatApp.git
   cd LlamaChatApp
   
2. **Install Node.js dependencies**
   cd server
    npm install

3. **Install React native and expo**
   add functions here..
   
5. **Install Ollama**
   npm install Ollama ithink
   
## Running the app
**ollama run llama3.1**
**node server.js**
let this run on whatever port it picks prly 3000, make sure to change IP address in AIService.js to the one you are active on. next command will display that for you.
**npx expo start** 
then launch your emulator of choice (use ios)

## Booya
More intructions will come as development continues

